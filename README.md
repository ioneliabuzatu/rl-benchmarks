# rl-benchmarks

https://hackmd.io/@92tLxRFMRF-iTbw8_IQU6Q/Sk6aw_wjP/edit

### TODO models benchmark

1. - [ ] DDPG - [paper](https://arxiv.org/abs/1509.02971), [code]()
4. - [ ] TD3 - [paper](https://spinningup.openai.com/en/latest/algorithms/td3.html), [code]()
5. - [ ] SAC - [paper](https://arxiv.org/pdf/1801.01290.pdf), [code]()
2. - [ ] DPG - [paper](https://deepmind.com/research/publications/deterministic-policy-gradient-algorithms), [code]()
3. - [ ] DQN - [paper](https://arxiv.org/abs/1312.5602), [code]()

### TODO benchmark environments 

1. - [ ] [CartPole-v1](https://gym.openai.com/envs/CartPole-v1)
2. - [ ] [Pendulum-v0](https://gym.openai.com/envs/Pendulum-v0)
1. - [ ] [Ant-v2](https://gym.openai.com/envs/Ant-v2)
1. - [ ] [Humanoid-v2](https://gym.openai.com/envs/Humanoid-v2)
1. - [ ] [Swimmer-v2](https://gym.openai.com/envs/Swimmer-v2)
1. - [ ] [Walker2d-v2](https://gym.openai.com/envs/Walker2d-v2)

### show results with [experiment_buddy](https://github.com/ministry-of-silly-code/experiment_buddy)


